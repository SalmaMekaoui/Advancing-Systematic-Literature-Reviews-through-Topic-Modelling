{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48be1f0",
   "metadata": {},
   "source": [
    "# Importing the required libraraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ec15c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import text\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pdfminer.high_level import extract_text\n",
    "from spacy.lang.de.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a03b3",
   "metadata": {},
   "source": [
    "## Format Conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351d87bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting pdf to text\n",
      "done in 0.000s.\n"
     ]
    }
   ],
   "source": [
    "files = glob('Folder where articles existe/*.pdf',recursive = True)\n",
    "i=0\n",
    "print(\"converting pdf to text\")\n",
    "t0 = time()\n",
    "for file in files:\n",
    "    print(file)\n",
    "    text = extract_text(file)\n",
    "    output_file = open(r\"Folder where data will be stored after transformation\\\\\"+str(i+10)+\".txt\", \"w\", encoding=\"utf-8\")\n",
    "    output_file.write(text)\n",
    "    output_file.close()\n",
    "    print(i+10)\n",
    "    i=i+1\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa00bf2",
   "metadata": {},
   "source": [
    "## Primary results of topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4fe22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(int(n_components/5), 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        #argsort : put element in ascendent or desendent order in this case descendent\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        ax = axes[topic_idx]\n",
    "        #create a graphe where we can the name of features with their weights\n",
    "        ax.barh(top_features, weights, height=0.7,color=['pink','black'])\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ebcba",
   "metadata": {},
   "source": [
    "## Words Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6262b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data(text):\n",
    "    #remove urls\n",
    "    text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text, flags=re.MULTILINE)\n",
    "    #remove emails\n",
    "    text = re.sub('\\w*\\._?\\w*\\@\\w*\\.\\w*', ' ', text)\n",
    "    text = re.sub('\\w*\\-*\\w*\\@\\w*\\.\\w*', ' ', text)\n",
    "    text = re.sub('\\w*\\@\\w*\\.\\w*', ' ', text)\n",
    "    #put text in lower letters\n",
    "    text = text.lower()\n",
    "    # remove text in square brackets\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    # remove text in parenthesis\n",
    "    text = re.sub('\\(.*?\\)', ' ', text)\n",
    "    #remove special caracteres.\n",
    "    text = re.sub('[ÔøΩ‚àö ‚Éí‚àëœÜ¬©‚àÄùõø‚Üí‚àà‚Ä¢‚àó]', ' ', text)\n",
    "    #remore numbers\n",
    "    text = re.sub('\\d*\\s', ' ', text)\n",
    "    text = re.sub('\\s\\d*', ' ', text)\n",
    "    #remove punctuation (in addition special caracters)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    #remove one letter\n",
    "    text = re.sub('\\s\\w\\s', ' ', text)\n",
    "    #remove some words that means nothing\n",
    "    text = re.sub(r'\\bieee\\b', ' ', text)\n",
    "    text = re.sub(r'\\bcontents lists available at sciencedirect\\b', ' ', text)\n",
    "    text = re.sub(r'\\blimerick\\b', ' ', text)\n",
    "    text = re.sub(r'\\bxplore\\b', ' ', text)\n",
    "    text = re.sub(r'\\bpp\\b', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348870ea",
   "metadata": {},
   "source": [
    "## Storage of TXT files to the new location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5f2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "filenames = glob('Folder where data will be stored after transformation/*.txt')\n",
    "\n",
    "for filename in filenames:\n",
    "    file = open(filename, \"r+\", encoding=\"utf-8\")\n",
    "    articles.append(cleaning_data(cleaning_data(file.read().rstrip())))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d88065",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a65532",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "article_words1=[]\n",
    "total_words=0\n",
    "articlesL=[]\n",
    "for article in articles:\n",
    "    article_words1=article.split()\n",
    "    article = ' '.join([WordNetLemmatizer().lemmatize(w) for w in article_words1])\n",
    "    article_words1=article.split()\n",
    "    article = ' '.join([stemmer.stem(w) for w in article_words1])\n",
    "    articlesL.append(article)\n",
    "    total_words=total_words+len(article_words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c1259",
   "metadata": {},
   "source": [
    "## The percentage of each topic‚Äôs presence in a document. + Application interface: number of occurrences of each word in topic 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4520609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topics_occurancy(model, feature_names):\n",
    "    D=0\n",
    "    for article in articlesL:\n",
    "        F=0\n",
    "        rate=[]\n",
    "        name=[]\n",
    "        table=[]\n",
    "        left_coordinates=[]\n",
    "        D=D+1\n",
    "        print(\"Document number:\",D)\n",
    "        total_number=0\n",
    "        article_words=article.split()\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "            top_features = [feature_names[i] for i in top_features_ind]\n",
    "            i=0\n",
    "            R=0\n",
    "            table_words=[]\n",
    "            word_rate=[]\n",
    "            left_coordinate=[]\n",
    "            z=0\n",
    "            for word in top_features:\n",
    "                z=z+1\n",
    "                left_coordinate.append(z)\n",
    "                k=0\n",
    "                table_words.append(word)\n",
    "                total_one_word=0\n",
    "                for i in range(len(article_words)):\n",
    "                    if word == article_words[i]:\n",
    "                        k=k+1\n",
    "                        total_one_word=total_one_word+1\n",
    "                word_rate.append(total_one_word)     \n",
    "                R=R+k\n",
    "            table.append(R)\n",
    "            total_number+=R\n",
    "            plt.figure(figsize =(20, 5))\n",
    "            plt.bar(left_coordinate,word_rate,tick_label=table_words,width=0.8,color=['yellow','black'])\n",
    "            plt.xlabel('words')\n",
    "            plt.ylabel('number of occurrences')\n",
    "            plt.title(f\"topic {topic_idx+1}\")\n",
    "            plt.show()\n",
    "        S=0\n",
    "        y=0\n",
    "        for topic_ind, topic in enumerate(model.components_):\n",
    "            y=y+1\n",
    "            left_coordinates.append(y)\n",
    "            S=S+(100/total_number)*table[topic_ind]\n",
    "            F=(100/total_number)*table[topic_ind]\n",
    "            rate.append(F)\n",
    "            name.append(f\"Topic {topic_ind+1}\")\n",
    "        plt.figure(figsize =(topic_ind, 5))\n",
    "        plt.bar(left_coordinates,rate,tick_label=name,width=0.8,color=['black','blue'])\n",
    "        plt.xlabel('TOPICS')\n",
    "        plt.ylabel('percentage')\n",
    "        plt.title(f\"The percentage of each topic‚Äôs presence in document {D}\")\n",
    "        plt.show()\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8cc016",
   "metadata": {},
   "source": [
    "# TF-IDF word embedding application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950e79be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for Topic modeling...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39mn_features, stop_words\u001b[38;5;241m=\u001b[39m stopset)\n\u001b[0;32m      4\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m----> 5\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(articlesL)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone in \u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (time() \u001b[38;5;241m-\u001b[39m t0))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1289\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1287\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1289\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1290\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1291\u001b[0m         )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "stopset = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "print(\"Extracting tf-idf features for Topic modeling...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=n_features, stop_words= stopset)\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(articlesL)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad51c5",
   "metadata": {},
   "source": [
    "## results representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" * 2,\"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"% (n_samples, n_features),)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components,max_iter=10,learning_method=\"online\",learning_offset=50.0,random_state=0,)\n",
    "t0 = time()\n",
    "lda.fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"done in %0.3fs.\"% (time() - t0)) \n",
    "plot_top_words(lda, tfidf_feature_names, n_top_words, \"Primary results of topic modeling.\")\n",
    "topics_occurancy(lda, tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d48ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e74f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b4c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
